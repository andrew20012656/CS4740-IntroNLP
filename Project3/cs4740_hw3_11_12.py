# -*- coding: utf-8 -*-
"""CS4740_HW3_11_12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SrOzPKaeDLAezqh9iyyhhSffHJZfwd7W

# **Homework 3: Semantic Role Labeling**
## CS4740/5740 Fall 2022 

## Grouping deadline: Oct. 30th 11:59pm on CMS (site will open soon)

## Milestone 1 Due: Fri Nov. 4th, 11:59pm  on CMS (submit your first model test set result on Kaggle)
## Project Submission Due: Tuesday Nov 15th, 11:59pm

**Names:**

**Netids:**


**Editing your version of this notebook:** One partner should make a copy of this notebook and share it with your partner.  **However**, because of synchronization issues (even though Colab works with Google Drive), changes made in this notebook at the same time from different computers/browser windows may not save. We will go so far as to recommend that you close the tab with this notebook when you are not working on it so your partner doesn't face sync issues.

**Collaboration policy:** Please be sure to check the collaboration policy on the [course website](https://courses.cs.cornell.edu/cs4740/2022fa/)!

**Coding cells:** Any cells that require you to code will have the `### TODO : [description] ###` comment, where the `[description]` varies per cell.

**Cell Outputs:** For grading (aka to help us give you partial credit), please ensure the final cell outputs are present for all cells in the notebook you submit.

**Teams/groups:** Everyone resets to a "singleton" when each new assignment is released.  If you want to have a partner for HW3, regardless of whether it's a previous homework partner or a new one, you must do the CMS invite/accept process anew.

Code of Academic Integrity:  **Do not copy code from online or share code with students other than your designated partner**. We will be running programs to detect plagiarism.

Kaggle for HW3: https://www.kaggle.com/t/fa506c5c444f43469a693f8366bd23a2

**What to submit**:
upload your two models and jupyter notebook to **CMS** and pdf to Gradescope

# **Introduction** 
---

Semantic Role Labeling (SRL) is the task of automatically labelling the semantic roles of each argument according to each predicate in a passage. **Predicates** (e.g., bought, sell, purchasing) represent events, and each sentence can have more than one predicate. **Semantic roles** express the abstract roles that predicate arguments can take in the event. Typical semantic arguments include Agent, Patient, and Instrument, and also adjunctive arguments indicating Locative, Temporal, Manner, Cause, and other aspects. Recognizing and labeling semantic arguments is a key task for answering "Who", "When", "What", "Where", and "Why" questions in Information Extraction, Question Answering, Summarization, and, in general, in all NLP tasks in which some kind of semantic interpretation is needed.

The following sentence, exemplifies the annotation of semantic roles:

[Arg0 He ] would n't accept [Arg1 anything of value ] from [Arg2 those he was writing about ] .

Here, the roles for the **predicate 'accept'** (that is, the roleset of the predicate) are defined in the PropBank Frames scheme as: \\

> **Arg0**: Agent \\
**Arg1**: Patient \\
**Arg2**: Instrument, benefactive, attribute \\
**ArgM-LOC**: Locative modifiers indicate where some action takes place. The notion of a locative is not
restricted to physical locations, but abstract locations are being marked as LOC as well, as "[in his speech]-LOC he was talking about â€¦".  \\
**ArgM-TMP**: Temporal ArgMs show when an action took place, such as "in 1987", "last Wednesday","soon" or "immediately". Also included in this category are adverbs of frequency (eg. often
always, sometimes), adverbs of duration
(for a year/in an year), order (eg. first), and repetition (eg. again)..  

For this homework, the predicate is given for every sentence. Your task is to implement 2 models to predict A0/A1/A2/ArgM-LOC/ArgM-TMP.

## **Advice**

---
1. We strongly encourage you to enable GPU support (Google Colab provides GPUs if you don't have them locally). To do so, go to `Runtime` --> `Change Runtime Type` --> Dropdown box below `Hardware Accelerator` --> `GPU`. This will help your models train *much* faster!
2. Please read through the entire notebook before you start coding. That might inform your code structure.
3. An assignment outline and grading breakdown (subject to minor adjustments) is found below; please consult it.

<a name="outline"></a>
## **Assignment outline and grading breakdown**
- [Part 1](#part1) Preparing Data
- [Part 2](#part2) LSTM Encoder implementation
  - [Implement LSTM Encoder Model](#l2)
      - Initialization [4 pts]
      - Forward [10 pts]    
  - [Submission to Kaggle](#l3) [5 pts]
  - [Q2.1](#Q2.1) [3 pts]    
  - [Q2.2](#Q2.2) [3 pts]       
- [Part 3](#part3) SRL Model Implementation
  - [Implement Encoder-Decoder Model](#l4) 
    - Encoder
      - Initialization [3 pts]
      - Forward [10 pts]
    - Decoder 
      - Initialization [4 pts]
      - Forward [10 pts]
      - Step [10 pts]
  - [Submission to Kaggle](#l5) [5 pts]
  - [Q3.1](#Q3.1) [3 pts]
- [Part 4](#part4) [10 pts]
- [Part 5](#part5)
  - Outperforming our LSTM baseline on Kaggle [5 pts]
  - Outperforming our Encoder-Decoder baseline on Kaggle [10 pts]
- **Milestone 1 submission** [5 pts]

**A brief note on the Kaggle baselines:** To give you some breathing room for your implementation, our baselines are *not* perfect implementations. Correct implementations should therefore outperform our baselines.

<a name="part1"></a>
[[^^^]](#outline) 
# **Part 1: Preparing Data**

In this part, data will be converted into a format that can be passed through both of our models.
"""

# Installing gensim (if needed)
# !pip install -U gensim
!pip install sentencepiece

from collections import Counter, namedtuple
from itertools import chain
import json
import math
import os
from pathlib import Path
import random
import time
from tqdm.notebook import tqdm, trange
from typing import List, Tuple, Dict, Set, Union
import sys

import gensim
import nltk
import matplotlib.pyplot as plt 
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction
import numpy as np
import sentencepiece as spm
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torch.nn import init
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
import torch.nn.utils
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence


from tqdm.notebook import tqdm, trange

"""##1.1: Data Loading
First, we need to load in the data from our Google Drive. Please adjust the following code to fit your drive's organization.
"""

from google.colab import drive
drive.mount('/content/drive')
### TODO : please modify the line below with your drive organization ###
path = os.path.join(os.getcwd(), "drive", "MyDrive", "CS4740_HW3")
with open(os.path.join(path,'train.json'), 'r') as f:
     train = json.loads(f.read())

with open(os.path.join(path,'valid.json'), 'r') as f:
     val = json.loads(f.read())

with open(os.path.join(path,'test.json'), 'r') as f:
     test = json.loads(f.read())

"""## 1.2: Preprocess Data
Currently, the dataset has four fields: 
1. 'text' is a list of a list of tokens(string)
2. 'verb_index' represents the **relative position** of the **predicate verb** in the sentence. (Type: a list of integers)
 
> Example: He would n't accept anything of value from those he was writing about.

> Verb index: 3, because accept is the fourth word of the sentence (indexing starts from zero).


3. 'srl_label' is the semantic role label of every token. (Type: a list of a list of strings) \\

4. 'word_indices' is the index of every word token (same as howework 1&2, Type: a list of a list of integers)

We could try printing out the second training sentence as an example of the dataset. \\
The verb_index here is 4, which means the fifth word of the sentence ("coordinated") is the **predicate**.
"""

import pandas as pd
train = pd.read_json("./drive/MyDrive/CS4740_HW3/train.json")
val = pd.read_json("./drive/MyDrive/CS4740_HW3/valid.json")
test = pd.read_json("./drive/MyDrive/CS4740_HW3/test.json")

print(train['verb_index'][1])
print(train['text'][1])
print(train['words_indices'][1])
print(train['srl_frames'][1])

"""### **Converting Labels to Numerical Values**
In addition to converting our tokens to vector representations, we need to convert our labels to numerical representations. For example, say we have two labels: "O", "B-ARG0". We could numerically represent these labels in a dictionary as Label 0 and Label 1: `{"O":0, "B-ARG":1}`. The following function is implemented for you, and it encodes the labels present in the dataset. No modifications are necessary.

"""

srl_map = {'O': 0,
           'B-ARG0': 1,
           'I-ARG0': 2,
           'B-ARG1': 3,
           'I-ARG1': 4,
           'B-ARG2': 5,
           'I-ARG2': 6,
           'B-ARGM-LOC': 7,
           'I-ARGM-LOC': 8,
           'B-ARGM-TMP': 9,
           'I-ARGM-TMP': 10}

def encode_srl_category(category_data: List[List[str]])->List[List[int]]:
  """ Encoding SRL category from a list of strings to a list of integers

  Arguments: 
    category_data (list(list(str))): SRL categories

  Returns:
    encoded category (list(list(int))): Numerical conversions of SRL categories
  """
  encoded_category = []
  for srl_list in category_data:
    encoded_srl_list = []
    for srl in srl_list:
      try:
        encoded_srl_list.append(srl_map[srl])
      except:
        encoded_srl_list.append(0)
    encoded_category.append(encoded_srl_list)
  return encoded_category

"""We have also provided the `pad_sents` function, which pads all sentences to be the same length, specifically the length of the longest input sentence, using the provided `pad_token`. Finally, we have provided the `Vocab` class that represents the corpus as a `Vocab` object with several helper functions. These are used to preprocess the training set for you."""

def pad_sents(sents, pad_token):
    sents_padded = []

    max_len = max([len(sent) for sent in sents])
    sents_padded = [(sent + ([pad_token] * (max_len - len(sent)))) for sent in sents]

    return sents_padded

class Vocab(object):
    """ Vocabulary, i.e. structure containing either
    src or tgt language terms.
    """
    def __init__(self, word2id=None):
        """ Init Vocab Instance.
        
        :param word2id: dictionary mapping words 2 indices
        :type word2id: dict[str, int]
        """
        if word2id:
            self.word2id = word2id
        else:
            self.word2id = dict()
            self.word2id['<pad>'] = 0   # Pad Token
            self.word2id['<s>'] = 1 # Start Token
            self.word2id['</s>'] = 2    # End Token
            self.word2id['<unk>'] = 3   # Unknown Token
        self.unk_id = self.word2id['<unk>']
        self.id2word = {v: k for k, v in self.word2id.items()}

    def __getitem__(self, word):
        """ Retrieve word's index. Return the index for the unk
        token if the word is out of vocabulary.
        
        :param word: word to look up
        :type word: str
        :returns: index of word
        :rtype: int
        """
        return self.word2id.get(word, self.unk_id)

    def __contains__(self, word):
        """ Check if word is captured by Vocab.
        
        :param word: word to look up
        :type word: str
        :returns: whether word is in vocab
        :rtype: bool
        """
        return word in self.word2id

    def __setitem__(self, key, value):
        """ Raise error, if one tries to edit the Vocab directly.
        """
        raise ValueError('vocabulary is readonly')

    def __len__(self):
        """ Compute number of words in Voca.
        
        :returns: number of words in Vocab
        :rtype: int
        """
        return len(self.word2id)

    def __repr__(self):
        """ Representation of Vocab to be used
        when printing the object.
        """
        return 'Vocabulary[size=%d]' % len(self)

    def add(self, word):
        """ Add word to Vocab, if it is previously unseen.
        
        :param word: to add to Vocab
        :type word: str
        :returns: index that the word has been assigned
        :rtype: int
        """
        if word not in self:
            wid = self.word2id[word] = len(self)
            self.id2word[wid] = word
            return wid
        else:
            return self[word]

    def words2indices(self, sents):
        """ Convert list of words or list of sentences of words
        into list or list of list of indices.
        
        :param sents: sentence(s) in words
        :type sents: Union[List[str], List[List[str]]]
        :returns: sentence(s) in indices
        :rtype: Union[List[int], List[List[int]]]
        """
        if type(sents[0]) == list:
            return [[self[w] for w in s] for s in sents]
        else:
            return [self[w] for w in sents]

    def indices2words(self, word_ids):
        """ Convert list of indices into words.
        
        :param word_ids: list of word ids
        :type word_ids: List[int]
        :returns: list of words
        :rtype: List[Str]
        """
        return [self.id2word[w_id] for w_id in word_ids]

    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:
        """ Convert list of sentences (words) into tensor with necessary padding for 
        shorter sentences.
        
        :param sents: list of sentences (words)
        :type sents: List[List[str]]
        :param device: Device on which to load the tensor, ie. CPU or GPU
        :type device: torch.device
        :returns: Sentence tensor of (batch_size, max_sentence_length)
        :rtype: torch.Tensor
        """
        word_ids = self.words2indices(sents)
        sents_t = pad_sents(word_ids, self['<pad>'])
        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)
        return sents_var

    @staticmethod
    def from_corpus(corpus, size, remove_frac=None, freq_cutoff=None): 
        """ Given a corpus construct a Vocab.
        
        :param corpus: corpus of text produced by read_corpus function
        :type corpus: List[str]
        :param freq_cutoff: if word occurs n < freq_cutoff times, drop the word
        :type freq_cutoff: int
        :returns: Vocab instance produced from provided corpus
        :rtype: Vocab
        """
        vocab_entry = Vocab()
        word_freq = Counter(chain(*corpus))
        if freq_cutoff is None:
            freq_cutoff = 1
        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]
        print('number of word types: {}, number of word types w/ frequency >= {}: {}'
              .format(len(word_freq), freq_cutoff, len(valid_words)))
        top_words = sorted(valid_words, key=lambda word: word_freq[word], reverse=True)
        if remove_frac is not None:
            size = len(top_words) - int(remove_frac * len(top_words))
            top_words = top_words[:size]
            print(f'number of unique words retained with remove_frac={remove_frac}: {len(top_words)}')
        for word in top_words:
            vocab_entry.add(word)
        return vocab_entry
    
    @staticmethod
    def from_subword_list(subword_list):
        """Given a list of subwords, construct the Vocab.
        
        :param subword_list: list of subwords in corpus
        :type subword_list: List[str]
        :returns: Vocab instance produced from provided list
        :rtype: Vocab
        """
        vocab_entry = Vocab()
        for subword in subword_list:
            vocab_entry.add(subword)
        return vocab_entry

"""# **Part 2: LSTM Encoder Model**

<a name="part2"></a>

We are using a single layer LSTM to predict Semantic Role Labels in our first model.

Input: word tokens $\vec{x}_1,\vec{x}_2, \dots, \vec{x}_k$ and index $i$ of the verb ($\vec{x}_i$) in the sentence \

In the neural network:
1.   Pass each word embedding to the LSTM and get the corresponding LSTM hidden layer.
2.   Get hidden state of the predicate in the sentence
2.   Concatenate the hidden states with the predicate hidden state.
3.   Pass the concatenated embedding through a linear layer to generate output





In this part, you need to implement an LSTM Encoder in PyTorch. Since you already understand how to implement RNNs from HW2, you are now allowed, **and encouraged** to use the torch.nn.LSTM function. https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html
(tip: read the documention and understand the parameter: "batch_first")

![hw3-1.png](data:image/png;base64,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)
"""

# Setting seed ***DO NOT MODIFY***
torch.manual_seed(123)

print('initialize train vocabulary ..')
src_vocab = Vocab.from_corpus(train['text'], 20000, remove_frac=0.3)

train_data = list(zip(train['text'],train['verb_index'],encode_srl_category(train['srl_frames'])))
val_data = list(zip(val['text'], val['verb_index'], encode_srl_category(val['srl_frames'])))

# Lambda to switch to GPU if available
get_device = lambda : "cuda:0" if torch.cuda.is_available() else "cpu"

"""## 2.1 Implementation

Your first task is to **implement the model below by finishing the #TODOs**.


<a name="l2"></a>
"""

from torch import lstm
class LSTMTagger(nn.Module):
    def __init__(self, src_vocab, embed_dim,
    hidden_dim, output_dim, vocab_size, num_layers=1):
        '''
        src_vocab: vocabulary of inputs (Class Vocab)
        embed_dim: dimension of word embedding
        hidden_dim: dimension of hidden layer
        output_dim: dimension of tagset_size
        vocab_size: vocabulary size
        num_layers: number of LSTM layers 
        '''
        super(LSTMTagger, self).__init__()
        self.src_vocab = src_vocab
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.embed_dim = embed_dim

        ### TODO : Initialize three linear layers:
              # 1. A word embedding layer
              # 2. A LSTM layer
              # 3. An output linear layer
        ### TODO : Initialize logsoftmax
        self.embedding = nn.Embedding(vocab_size, embed_dim, max_norm = True)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first = True)
        self.W = nn.Linear(2*hidden_dim, output_dim) ## W is the weight matrix for output layer
        self.softmax = nn.LogSoftmax(dim=1)

    def compute_Loss(self, criterion, predicted_vector, gold_label):
        loss = 0
        for n in range(len(predicted_vector)): # batch size
            loss += criterion(predicted_vector[n], gold_label[n])	
        return loss

    def forward(self, source: List[List[str]], verb_indices:List[int]):
        ### GOALS : Write the forward function such that it processes sentences. 
        ### GOALS : Return output of the logsoftmax across all time steps
       
        #pad input sentences and conver to word index 
        source_padded = self.src_vocab.to_input_tensor(source, device=get_device())
        batch_size = source_padded.shape[0]        
        time_steps = source_padded.shape[1]

        #set up outputs dimension
        output = torch.zeros(batch_size, time_steps, self.output_dim).to(get_device())
        # output = torch.zeros(batch_size, time_steps, self.output_dim)

        #TODO1: Convert word index to embedding
        embeddings = self.embedding(source_padded)
        #TODO2: Pass inputs to the lstm layer
        lstms = self.lstm(embeddings)
        #TODO3: Get hidden state of verb in the sentence
        outputs, _= lstms
        verbs_hidden_states = torch.zeros(batch_size, self.hidden_dim).to(get_device())
        for i in range(batch_size):
          verbs_hidden_states[i,:] = outputs[i,verb_indices[i], :]

        #TODO4: Iterate over the time dimension: 
        #       - Concatenate verb hidden state to the hidden layer output of every token
        #       - Predict SRL tag distribution with output layer and logsoftmax
        for i in range(time_steps):
          input_t = torch.cat((outputs[:,i,:],verbs_hidden_states), 1)
          output[:,i,:] = self.softmax(self.W(input_t))
        return output


    def load_model(self, save_path):
        saved_model = torch.load(save_path)
        self.load_state_dict(saved_model.state_dict())

    def save_model(self, save_path):
        torch.save(self, save_path)

def batch_iter(data, batch_size, shuffle=False):
    """ Yield batches of input sentence, verb indices, target output labels 
    :param data: list of tuples containing source and target sentence. ie.
        (list of (src_sent, tgt_sent))
    :type data: List[Tuple[List[str], List[str], List[str]]]
    :param batch_size: batch size
    :type batch_size: int
    :param shuffle: whether to randomly shuffle the dataset
    :type shuffle: boolean
    """
    batch_num = math.ceil(len(data) / batch_size)
    index_array = list(range(len(data)))

    if shuffle:
        np.random.shuffle(index_array)

    for i in range(batch_num):
        indices = index_array[i * batch_size: (i + 1) * batch_size]
        examples = [data[idx] for idx in indices]

        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)
        srl = [e[0] for e in examples]
        verb_index = [e[1] for e in examples]
        target = [e[2] for e in examples]

        yield srl, verb_index, target

# Setting seed ***DO NOT MODIFY***
torch.manual_seed(123)

def evaluation(model, val_data, optimizer, criterion, batch_size=64):
  model.eval()
  loss = 0
  correct = 0
  total = 0
  batch = 0
  for (input_batch, verb_indices, expected_out) in tqdm(batch_iter(val_data, batch_size=batch_size, shuffle=True)):
    output = model.forward(input_batch, torch.tensor(verb_indices).to(get_device()))
    total += output.size()[0] * output.size()[1]
    _, predicted = torch.max(output, 2)
    expected_out = torch.tensor(pad_sents(expected_out,0))
    correct += (expected_out.to("cpu") == predicted.to("cpu")).cpu().numpy().sum()

    loss += model.compute_Loss(criterion, output.to("cpu"), expected_out.to("cpu"))
    batch += 1
  loss /= batch
  print("Validation Loss: " + str(loss.item()))
  print("Validation Accuracy: " + str(correct/total))
  print()
  return loss.item()

# Setting seed ***DO NOT MODIFY***
torch.manual_seed(123)

def train_epoch(model, train_data, optimizer, criterion,batch_size=64):
  model.train()
  total = 0
  batch = 0
  total_loss = 0
  correct = 0
  for (input_batch, verb_indices, expected_out) in tqdm(batch_iter(train_data, batch_size=batch_size, shuffle=True)):
    optimizer.zero_grad()
    batch += 1
    output = model.forward(input_batch, torch.tensor(verb_indices).to(get_device()))
    total += output.size()[0] * output.size()[1]
    _, predicted = torch.max(output, 2)

    expected_out = torch.tensor(pad_sents(expected_out,0))
    correct += (expected_out.to("cpu") == predicted.to("cpu")).cpu().numpy().sum()
    
    loss = model.compute_Loss(criterion, output.to("cpu"), expected_out.to("cpu")) 
    total_loss += loss.item()
    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), 1)
    optimizer.step() 
  print("Loss: " + str(total_loss/batch))
  print("Training Accuracy: " + str(correct/total))
  return total_loss/batch

# Setting seed ***DO NOT MODIFY***
torch.manual_seed(123)

def train_and_evaluate(number_of_epochs, model, train_data, val_data, criterion, min_loss=0, lr=.01):
  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=.9)
  loss_values = [[],[]]
  for epoch in trange(number_of_epochs, desc="Epochs"):
    cur_loss = train_epoch(model, train_data, optimizer, criterion)
    loss_values[0].append(cur_loss)
    cur_loss_val = evaluation(model, val_data, optimizer, criterion)
    loss_values[1].append(cur_loss_val)
    if cur_loss <= min_loss: return loss_values
  return loss_values

"""**Train your LSTM Encoder Model** with the following cell:"""

embed_dim = 64
hidden_dim = 64
weight = torch.ones(len(srl_map))
weight[0] = 0.25
print(weight)
criterion = nn.NLLLoss(weight=weight)
lstm = LSTMTagger(src_vocab, embed_dim, hidden_dim=hidden_dim, output_dim=len(srl_map), vocab_size=len(src_vocab)).to(get_device())
loss = train_and_evaluate(10, lstm, train_data, val_data, criterion, min_loss=0.1, lr= .05)

# lstm.save_model("./drive/MyDrive/CS4740_HW3/lstm_11_12.model") 
# lstm.save_model("./drive/MyDrive/CS4740_HW3/lstm_11_12.pth")

"""## 2.1 Get Entity level F1 score on the validation set

Run the cells below to calculate your F1 score on the validation set (no modifications needed):
"""

lstm.load_model("./drive/MyDrive/CS4740_HW3/lstm_final.pth")

def format_output_labels(token_labels, token_indices):
    """
    Returns a dictionary that has the labels (ARG0,ARG1,ARG2,TMP,LOC) as the keys, 
    with the associated value being the list of entities predicted to be of that key label. 
    Each entity is specified by its starting and ending position indicated in [token_indices].

    :parameter token_labels: A list of token labels 
    :type token_labels: List[String]
    :parameter token_indices: A list of token indices (taken from the dataset) 
                              corresponding to the labels in [token_labels].
    :type token_indices: List[int]
    """
    label_dict = {"ARG0":[], "ARG1":[], "ARG2":[], "LOC":[],"TMP":[]}
    prev_label = 'O'
    start = token_indices[0]
    for idx, label in enumerate(token_labels):
      curr_label = label.split('-')[-1]
      if label.startswith('B-') or curr_label != prev_label:
        if prev_label != 'O':
          label_dict[prev_label].append((start, token_indices[idx-1]))
        if curr_label != 'O':
          start = token_indices[idx]
        else:
          start = None
      
      prev_label = curr_label

    if start is not None and prev_label != 'O':
      label_dict[prev_label].append((start, token_indices[idx]))
    return label_dict

# Code for mean F1

import numpy as np

def mean_f1(y_pred_dict, y_true_dict):
    F1_lst = []
    for key in y_true_dict:
        TP, FN, FP = 0, 0, 0
        num_correct, num_true = 0, 0
        preds = y_pred_dict[key]
        trues = y_true_dict[key]
        for true in trues:
            num_true += 1
            if true in preds:
                num_correct += 1
            else:
                continue
        num_pred = len(preds)
        if num_true != 0:
            if num_pred != 0 and num_correct != 0:
                R = num_correct / num_true
                P = num_correct / num_pred
                F1 = 2*P*R / (P + R)
            else:
                F1 = 0      # either no predictions or no correct predictions
        else:
            continue
        F1_lst.append(F1)
    return np.mean(F1_lst)

#get validation output
inv_srl_map={srl_map[key]:key for key in srl_map}
val_predict = []
val_true = []
val_idx = []

for idx in range(len(val_data)):
  out = lstm.forward([val_data[idx][0]], torch.tensor([val_data[idx][1]]))
  _, predicted = torch.max(out, 2)

  len_sent = len(val_data[idx][0])
  result = predicted.cpu().numpy()[0]
  
  for t in range(len_sent):
    val_predict.append(inv_srl_map[result[t]])
    val_true.append(inv_srl_map[val_data[idx][2][t]])

  val_idx.extend(val['words_indices'][idx])

#get validation score
y_pred_dict = format_output_labels(val_predict, val_idx)
y_true_dict = format_output_labels(val_true, val_idx)

print(str(mean_f1(y_pred_dict, y_true_dict)))

# Save our model!
# Don't change saved model name here
lstm.save_model("lstm.pth")
lstm.load_model("lstm.pth")

"""<a name="l3"></a>
## 2.2 Submission to Kaggle

Using the best-performing LSTM, generate predictions for the test set, and submit them to Kaggle competition.

Below, we will use the same output output function from HW1&2: create_submission. This function submits given predicted tokens and associated token indices in the correct format. As such, we will use the format_output_labels functions from HW1 as well.

Submissions to Kaggle should be a CSV file consisting of five lines and two columns. The first line is a fixed header, and each of the remaining four lines corresponds to one of the four types of named entities. The first column is the label identifier Id (one of ARG0, ARG1, ARG2, LOC, TMP), and the second column Predicted is a list of entities (separated by single space) that you predict to be of that type. Each entity is specified by its starting and ending index (concatenated by a hypen) as given in the test corpus.

You can use the function `create_submission` that takes the list of predicted labels and the list of associated token indices as inputs and creates the the output CSV file at a specified path.

NOTE: Ensure that there are no rows with Id = "O" in your Kaggle Submission.

**Hint: The following code will be similar to what we did to get the entity level F1 score on the validation set.**
"""

#TODO: First, process the test data into a form the models can use.
reverse_srl_map = {v: k for k, v in srl_map.items()} 
test_data = list(zip(test['text'],test['verb_index'], encode_srl_category(train['srl_frames'])))

#TODO: Pass the test data into the model and generate data for the Kaggle submission.
#TODO: First, process the test data into a form the models can use.
def lstm_test(model):
  predictions = []
  for (input_batch, verb_indices,_) in batch_iter(test_data, batch_size = 1):
    output = model(input_batch, torch.tensor(verb_indices).to(get_device()))
    _, predicted = torch.max(output, 2)
    predictions.append(predicted.squeeze().cpu().numpy())
  flattened_predictions = [i for sent in predictions for i in sent]
  return flattened_predictions

lstm_flattened_test_predictions = [reverse_srl_map[prediction_index] for prediction_index in lstm_test(lstm)]
lstm_flattened_test_words_indices = [j for i in test['words_indices'] for j in i]

import csv

def create_submission(output_filepath, token_labels, token_inds):
    """
    :parameter output_filepath: The full path (including file name) of the output file, 
                                with extension .csv
    :type output_filepath: [String]
    :parameter token_labels: A list of token labels (eg. PER, LOC, ORG or MISC).
    :type token_labels: List[String]
    :parameter token_indices: A list of token indices (taken from the dataset) 
                              corresponding to the labels in [token_labels].
    :type token_indices: List[int]
    """
    label_dict = format_output_labels(token_labels, token_inds)
    with open(output_filepath, mode='w') as csv_file:
        fieldnames = ['Id', 'Predicted']
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        for key in label_dict:
            p_string = " ".join([str(start)+"-"+str(end) for start,end in label_dict[key]])
            writer.writerow({'Id': key, 'Predicted': p_string})

#TODO: Call create_submission with the labels your model predicted.
create_submission("./drive/MyDrive/CS4740_HW3/lstm_predictions_11_12.csv",lstm_flattened_test_predictions, lstm_flattened_test_words_indices)

"""## 2.3 Questions

<a name="Q2.1"></a>
### **Q2.1**
What is the benefit of using LSTM compared to RNN?

#### A2.1

The LSTM can handle the vanishing gradient problem-the loss of information over a long period-associated with RNN model. More specifically, the LSTM solves the problem by forgetting the useless data in the network as it proceeds.

<a name="Q2.2"></a>
### **Q2.2**

Why do we need to concatenate the hidden states of predicate to the hidden layer of every token in the first model?

#### A2.2
One possible reason is that it's hard to predict whether a word token or a phrase is some argument for the predicate in this sentence without concatenating the predicate with that word or phrase.

# **Part 3: Encoder-Decoder Model**

<a name="part3"></a>
We can convert the Semantics Role Labeling task into the format of Question and Answer task.

> For example: [Arg0 He] would n't accept [Arg1 anything of value] from [Arg2 those he was writing about] .  
Given the input sentence **"He would n't accept  anything of value from those he was writing about ."** and predicate **"accept"**, we want to know what is ARG0, ARG1, ARG2, ARGM_TMP, ARGM_LOC in this sentence. Therefore, we can convert each example into 5 question and answer pairs. (The format for the inputs is *predicate [SEPT] setence label we want to find*, where [SEPT] is a separator token. The output is the sequence that corresponds with that label, or empty if that label is not in this sentence.) \
-> Input 1: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARG0\
-> Output 1: $<s>$ He $</s>$\
-> Input 2: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARG1\
-> Output 2: $<s>$ anything of value $</s>$
-> Input 3: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARG2\
-> Output 3: $<s>$ $</s>$ (Explanation: because there's no ARG2 in this sentence) \
-> Input 4: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARGM-TMP\
-> Output 4: $<s>$ $</s>$  \
-> Input 5: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARGM-LOC\
-> Output 5: $<s>$ $</s>$ 

Given the input, we want to use a seq2seq model to predict output. In this section, we describe the training procedure for the proposed encoder-decoder system, which uses a Bidirectional LSTM Encoder and a Unidirectional LSTM with attention Decoder. We'll recap the theoretical component here and in the modules where you are writing code, we will repeat the steps more explicitly in an algorithmic manner.

<Insert diagram here>

Given a sentence in the source language, we look up the word embeddings from an embeddings matrix, yielding $x_1,\dots, x_n$ ($x_i \in R^{e}$), where n is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional encoder, yielding hidden states and cell states for both the forwards (â†’) and backwards (â†) LSTMs. The forwards and backwards versions are concatenated to give hidden states $h_i^{enc}$ and cell state $c_i^{enc}$:


$$h_i^{enc} = [\overrightarrow{h_i^{enc}}; \overleftarrow{h_i^{enc}}] \text{ where }h_i^{enc} \in R^{2h}, \overrightarrow{h_i^{enc}}, \overleftarrow{h_i^{enc}} \in R^{h}$$

$$c_i^{enc} = [\overrightarrow{c_i^{enc}}; \overleftarrow{c_i^{enc}}] \text{ where }c_i^{enc} \in R^{2h}, \overrightarrow{c_i^{enc}}, \overleftarrow{c_i^{enc}} \in R^{h}$$


We then initialize the decoderâ€™s first hidden state $h_0^{dec}$ with a linear projection of the encoderâ€™s final hidden state

$$h_0^{dec} = W_h[\overrightarrow{h_n^{enc}}; \overleftarrow{h_0^{enc}}] \text{ where }h_0^{dec} \in R^{h}, W_h \in R^{h \times 2h}$$

And first cell state $c_0^{dec}$ with a linear projection of the encoderâ€™s final cell state

$$c_0^{dec} = W_c[\overrightarrow{c_n^{enc}}; \overleftarrow{c_0^{enc}}] \text{ where }c_0^{dec} \in R^{h}, W_c \in R^{h \times 2h}$$

With the decoder initialized, we must now feed it a target sentence. On the $t^{th}$ step, we look up the embedding for the $t^{th}$ word, $y_t \in R^{e}$. We then concatenate $y_t$ with the combined-output vector $o_{tâˆ’1} \in R^{h}$ from the previous timestep (we will explain in detail what this is later, but it is just the output from the previous step) to produce $\bar{y_t} \in R^{e+h}$. Note that for the first target (i.e. the start token), $o_0$ is usually a zero-vector (but it can be random or a learned vector as well). We then feed $y_t$ as input to the decoder.

$$ h_t^{dec} = Decoder(\bar{y_t}, (h_{t-1}^{dec},c_{t-1}^{dec}))\text{ where }h_{t-1}^{dec} âˆˆ R^{h}$$

We can take the decoder hidden state $h_t^{dec}$ and concatenate with attention context vector $a_t$
$$u_t = [h_t^{dec},a_t] \in R^{3h}$$

Then we pass $u_t$ through a linear layer to obtain our combined-output vector $v_t$:

$$v_t = W_v u_t \text{ where } W_v \in R^{h \times 3h}, v_t \in R^{h}$$

Then, we produce a probability distribution $P_t$ over target words at the $t^{th}$ timestep.

$$P_t = Softmax(W_{v_{target}} v_t) \text{ where }P_t \in R^{V_{target}}, W_{v_{target}}\in R^{V_{target} \times h}$$


Here, $V_{target}$ is the size of the target vocabulary. Finally, to train the network we then compute the softmax cross entropy loss between $P_t$ and $g_t$, where $g_t$ is the one-hot vector of the target word at timestep t:

$$Loss(Model) = CrossEntropy(P_t, g_t)$$

Now that we have described the model, letâ€™s try implementing it for the SRL task.

**How we get $a_t$ attention context vector:**

At the beginning of the decoder step, first we need to project encoder hidden states from $ R^{2h}$ to $ R^{h}$

$$h_i^{enc-projection} = W_a h_i^{enc}, W_a \in R^{h \times 2h}, \forall i$$

Then at each decoder step, we compute dot product similarity between $h_t^{dec}$ and $h_i^{enc-projection}, \forall i$:
$$score(h_t^{dec},h_i^{enc-projection}) = h_t^{dec} \cdot h_i^{enc-projection}$$
Softmax the scores to create vector of weights:
$$\alpha_t = softmax(score(h_t^{dec},h_i^{enc-projection}),\forall i)$$
Take the weighted average over all encoder hidden states
$$a_t = \alpha_t \cdot h_t^{enc} \in R^{2h}$$

## 3.1 Preprocess the data

The code here converts raw dataset to the input and output format mentioned above. No modifications necessary.
"""

srl_frames = ["ARGM-TMP", "ARG0", "ARG1", "ARG2", "ARGM-LOC"]
def get_srl_frames_indices(token_labels, token_indices):
    label_dict = {"ARGM-TMP":[], "ARG0":[], "ARG1":[], "ARG2":[], "ARGM-LOC":[]}
    prev_label = 'O'
    start = token_indices[0]
    for idx, label in enumerate(token_labels):
      curr_label = '-'.join(label.split('-')[1:]) if label != 'O' else 'O'
      if label.startswith("B-") or (curr_label != prev_label and curr_label != "O"):
        if prev_label != "O":
          label_dict[prev_label].append((start, token_indices[idx-1]))
        start = token_indices[idx]
      elif label == "O" and prev_label != "O":
        label_dict[prev_label].append((start, token_indices[idx-1]))
        start = None
      prev_label = curr_label
    if start is not None and prev_label != 'O':
      label_dict[prev_label].append((start, token_indices[idx-1]))
    return label_dict

SEPT = 'SEP_T'
def generate_source_corpus(source_text: List[List[str]], source_verb: List[int]):
  assert len(source_text) == len(source_verb)
  return [[source_text[i][source_verb[i]]]+ [SEPT] + [token for token in source_text[i]] + [arg] for i in range(len(source_text)) for arg in srl_frames]

def generate_target_corpus(source_text: List[List[str]], source_verb: List[int], source_srl: List[List[str]], source_indices: List[List[str]]):
  assert len(source_text) == len(source_verb)
  assert len(source_text) == len(source_srl)

  ans = []
  for i in range(len(source_text)):
    text = source_text[i]
    verb = text[source_verb[i]]
    srl = source_srl[i]
    indices = source_indices[i]
    indice_start = indices[0]
    label_dict = get_srl_frames_indices(srl, indices)
    for key in label_dict.keys():
      arg_lst = []
      for arg_idx in label_dict[key]:
        arg_lst += text[(arg_idx[0] - indice_start):(arg_idx[1]- indice_start +1)]
      ans += [['<s>'] +[token for token in arg_lst] + ['</s>']]

  return ans

"""The following cell builds a combined vocab dictionary for input (source) and output (target). Both will share the same vocabulary. """

train_src_corpus = generate_source_corpus(train['text'], train['verb_index'])
train_tgt_corpus = generate_target_corpus(train['text'], train['verb_index'], train['srl_frames'], train['words_indices'])
train_data = list(zip(train_src_corpus, train_tgt_corpus))

val_src_corpus = generate_source_corpus(val['text'], val['verb_index'])
val_tgt_corpus = generate_target_corpus(val['text'], val['verb_index'], val['srl_frames'], val['words_indices'])
val_data = list(zip(val_src_corpus, val_tgt_corpus))

#generate src_vocab
src_vocab = Vocab.from_corpus(np.array(train_src_corpus + val_src_corpus+train_tgt_corpus+val_tgt_corpus), 20000, remove_frac=0.3)
tgt_vocab = src_vocab

"""You can print out our converted inputs & outputs here:"""

print(f"train['text'] is {train['text'][1]}")
print(f"train['srl_frames'] is {train['srl_frames'][1]}")
print(f'train_src_corpus is {train_src_corpus[7]}')
print(f'train_tgt_corpus is {train_tgt_corpus[7]}')

"""The following cell loads pretrained GloVe embeddings and stores the embedding for each word in the vocabulary to `src_embeddings`. These embeddings will be used to initialize the learned embeddings in the models:"""

import gensim.downloader as api
model= api.load("glove-wiki-gigaword-300")
torch.manual_seed(1)
#obtain src_embeddings
src_embeddings = []
for i, word in enumerate(src_vocab.word2id.keys()):
    try: 
        src_embeddings.append(model[word])
    except:
        src_embeddings.append(torch.rand(300))

src_embeddings = np.stack(src_embeddings, 0)
src_embeddings = torch.from_numpy(src_embeddings)

Hypothesis = namedtuple('Hypothesis', ['value', 'score'])

"""<a name="l4"></a>
## 3.2 Implementation
Your next task is to **implement the Encoder-Decoder model  by finishing the #TODOs**.
"""

class Encoder(nn.Module):
    def __init__(self, embed_size, hidden_size, source_embeddings):
        """
        """
        super(Encoder, self).__init__()
        self.hidden_size = hidden_size
        self.embed_size = embed_size
        self.embedding = source_embeddings


        ### YOUR CODE HERE (~3 Lines)
        ### TODO - Initialize the following variables:
        ###     self.encoder (Bidirectional LSTM with bias)
        ###     self.h_projection (Linear Layer with bias),called W_{h} above.
        ###     self.c_projection (Linear Layer with bias),called W_{c} above.
        self.encoder = nn.LSTM(embed_size, hidden_size, bias = True, bidirectional = True, batch_first = True)
        self.h_projection = nn.Linear(2*hidden_size, hidden_size)
        self.c_projection = nn.Linear(2*hidden_size, hidden_size)

        
    def forward(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:

        ### YOUR CODE HERE 
        ### TODO:
        ###     1. Construct Tensor `X` by embedding the input. The result should have shape (b, src_len, e)
        ###         b = batch size, src_len = maximum source sentence length, e = embedding size. Note
        ###         that there is no initial hidden state or cell for the decoder.
        ###         Note: you should study the equations/mathematical definitions above to determine
        ###         what some of these values should be.  The same holds throughout.
        ###     2. Compute `enc_hiddens`, `last_hidden`,  `last_cell_state` by applying the LSTM encoder to `X`. 
        ###     3. Compute 
        ###         - `init_decoder_hidden`:
        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.
        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).
        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.
        ###             This is h_0^{dec} in above in the writeup. Here b = batch size, h = hidden size
        ###         - `init_cell_hidden`:
        ###             `last_cell_state` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.
        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).
        ###             Apply the c_projection layer to this in order to compute init_decoder_hidden.
        ###             This is c_0^{dec} in above in the writeup. Here b = batch size, h = hidden size        

        enc_hiddens, dec_init_state = None, None
        
        #TODO 1 
        X = self.embedding(source_padded)
        X = nn.utils.rnn.pack_padded_sequence(X, source_lengths, batch_first=True)

        #TODO 2
        enc_hiddens, (last_hidden, last_cell_state) = self.encoder(X) 
        (enc_hiddens, _) = nn.utils.rnn.pad_packed_sequence(enc_hiddens, batch_first=True)

        #TODO3 concatenate last hidden embed from both direction and with a linear projection 
        before_hidden_projection = torch.cat((last_hidden[0], last_hidden[1]), 1)
        init_decoder_hidden = self.h_projection(before_hidden_projection)

        #TODO4 concatenate last cell state from both direction and with a linear projection 
        before_cell_projection = torch.cat((last_cell_state[0], last_cell_state[1]), 1)
        init_cell_state = self.c_projection(before_cell_projection)
        
        dec_init_state = (init_decoder_hidden, init_cell_state)

        return enc_hiddens, dec_init_state

class Decoder(nn.Module):
    def __init__(self, embed_size, hidden_size, target_embedding, device):
        """
        """
        super(Decoder, self).__init__()
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.device = device
        self.embedding = target_embedding
        output_vocab_size = self.embedding.weight.size(0)
        self.softmax = nn.Softmax(dim=1)
        self.att_projection = nn.Linear(in_features=self.hidden_size * 2,
                                        out_features=self.hidden_size,
                                        bias=False)

        ### YOUR CODE HERE (~3 lines)
        ###     self.decoder (LSTM Cell with bias)
        ###     self.combined_output_projection (Linear Layer with no bias), called W_{v} above.
        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{target} above.
        ### You may find some of these functions useful:
        ###     LSTM Cell:
        ###     https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html
        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size, bias = True)
        self.combined_output_projection = nn.Linear(3*hidden_size, hidden_size, bias = False)
        self.target_vocab_projection = nn.Linear(hidden_size, output_vocab_size, bias = False)

    def step(self, Ybar_t: torch.Tensor,
            dec_state: Tuple[torch.Tensor, torch.Tensor],
            enc_hiddens: torch.Tensor,
            enc_hiddens_proj: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:
        """ Compute one forward step of the LSTM decoder, including the attention computation.

        :param Ybar_t: Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,
                                where b = batch size, e = embedding size, h = hidden size.
        :type Ybar_t: torch.Tensor
        :param dec_state: Tensors with shape (b, h), where b = batch size, h = hidden size.
                Tensor is decoder's prev hidden state
        :type dec_state: torch.Tensor
        :param enc_hiddens: Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,
                                    src_len = maximum source length, h = hidden size.
        :type enc_hiddens: torch.Tensor
        :param enc_hiddens_proj: Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),
                                    where b = batch size, src_len = maximum source length, h = hidden size.
        :type enc_hiddens_proj: torch.Tensor

        :returns dec_state: Tensors with shape (b, h), where b = batch size, h = hidden size.
                Tensor is decoder's new hidden state
        returns combined_output: Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.
        """

        combined_output = None

        ### TODO 1:
        ###     1. Apply the decoder to `Ybar_t` and `dec_state` to obtain the new dec_state.
        # print("Y_bar_t shape" + str(Ybar_t.shape))
        # print("dec_state shape " + str(dec_state[0].shape))
        # print("dec_state shape " + str(dec_state[1].shape))
        dec_state = self.decoder(Ybar_t, dec_state)
        (dec_hidden, dec_cell) = dec_state ## dec_hidden dim (b, h)

        ### TODO 2(Attention Step):
        ###     1. Use dot product to calculate similarity between enc_hiddens_proj and dec_hidden,
        ###        and then take softmax (this is the attention weight alpha_t)
        ###     2. Dot product attention weight with enc_hiddens to get weighted context embedding a_t
        ###     3. U_t = Concate dec_hidden and a_t 

        ## dec_hidden dim (b, h) and enc_hiddens_proj dim (b, src_len, h)
        alpha_t = self.softmax(torch.bmm(enc_hiddens_proj, torch.unsqueeze(dec_hidden, 2))) ## alpha_t shape (b, src_len, 1)
        # print("alpha_t shape is : " + str(alpha_t.shape))
        # print("enc_hidden shape is : " + str(enc_hiddens.shape))
        a_t = torch.squeeze(torch.bmm(torch.unsqueeze(torch.squeeze(alpha_t, 2), 1), enc_hiddens), 1) ## dim (b, h * 2)
        # print("a_t shape is " + str(a_t.shape))
        u_t = torch.cat((dec_hidden, a_t), 1) ## dim (b, h*3)
        # print("u_t shape is " + str(u_t.shape))
        ### TODO 3:
        ###     1. Apply the combined output projection layer to U_t to compute tensor V_t
        V_t = self.combined_output_projection(u_t)

        combined_output = V_t
        return dec_state, combined_output
    
    def forward(self, enc_hiddens: torch.Tensor,
                dec_init_state: torch.Tensor, target_padded: torch.Tensor) -> torch.Tensor:
        # Chop off the <END> token for max length sentences.
        target_padded = target_padded[:,:-1]

        dec_state = dec_init_state

        # Initialize previous combined output vector o_{t-1} as zero
        batch_size = enc_hiddens.size(0)
        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)

        # Initialize a list we will use to collect the combined output o_t on each step
        combined_outputs = []

        
        ### YOUR CODE HERE
        ### TODO:
        ###     1. Construct tensor `Y` by embedding the target sentences. The result should have shape (b, tgt_len, e)
        ###         where b = batch size, tgt_len = maximum target sentence length, e = embedding size.
        ###     2. Construct enc_hiddens_proj by using self.att_projection to project enc_hiddens to shape (b, src_len, h)
        ###     3. Iterate over the time dimension of Y.
        ###         Within the loop, this will give you Y_t of shape (b, 1, e) where b = batch size, e = embedding size.
        ###             - Squeeze Y_t into a tensor of dimension (b, e). 
        ###             - Construct Ybar_t by concatenating Y_t with o_prev on their last dimension
        ###             - Use the step function to compute the the Decoder's next (cell, state) values
        ###               as well as the new combined output o_t.
        ###             - Append o_t to combined_outputs
        ###             - Update o_prev to the new o_t.
        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of
        ###         tensors shape (b, h), to a single tensor shape (b, tgt, h)
        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.
        ### 
        ### Note:
        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze
        ###      over. Otherwise, you will remove the batch dimension accidentally.
        ###   
        ### You may find some of these functions useful:
        ###     Zeros Tensor:
        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros
        ###     Tensor Dimension Squeezing:
        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze
        ###     Tensor Concatenation:
        ###         https://pytorch.org/docs/stable/torch.html#torch.cat


        #TODO 1:
        Y = self.embedding(target_padded)
        # print("[OK] The shape of Y embedding in decode is " + str(Y.shape))

        #TODO 2:
        enc_hiddens_proj = self.att_projection(enc_hiddens)
        # print("The shape of enc_hiddens_proj in decode is " + str(enc_hiddens_proj.shape))
        
        #TODO 3:
        for i in range(Y.shape[1]):
            y_t = torch.squeeze(Y[:, i, :], 1)
            y_bar_t = torch.cat((y_t, o_prev), 1)
            dec_state, combined_output = self.step(y_bar_t, dec_state, enc_hiddens, enc_hiddens_proj)
            combined_outputs.append(combined_output)
            o_prev = combined_output
        # print("Combined outputs len" + str(len(combined_outputs)))
        # print("Combined output size " + str(combined_outputs[0].shape))

        #TODO 4:
        combined_outputs = torch.stack(combined_outputs, 1)
        # print("Combined outputs len" + str(len(combined_outputs)))
        return combined_outputs

"""The following class puts together the LSTM Encoder and Decoder with several helper methods that allow the model to generate outputs. No modifications are necessary."""

class SRL(nn.Module):
    """ Simple Semantic Role Labelling Model with:
        - Bidrectional LSTM Encoder
        - Unidirection LSTM Decoder
    """
    def __init__(self, embed_size, hidden_size, src_vocab, tgt_vocab, device=torch.device("cpu"), pretrained_source=None,pretrained_target=None,):
        """ Init SRL Model.

        :param embed_size: Embedding size (dimensionality)
        :type embed_size: int
        :param hidden_size: Hidden Size, the size of hidden states (dimensionality)
        :type hidden_size: int
        :param src_vocab: Vocabulary object containing src language
        :type src_vocab: Vocab
        :param tgt_vocab: Vocabulary object containing tgt language
        :type tgt_vocab: Vocab
        :param device: torch device to put all modules on
        :type device: torch.device
        :param pretrained_source: Matrix of pre-trained source word embeddings
        :type pretrained_source: Optional[torch.Tensor]
        :param pretrained_target: Matrix of pre-trained target word embeddings
        :type pretrained_target: Optional[torch.Tensor]
        """
        super(SRL, self).__init__()
        self.device=device
        self.embed_size = embed_size
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        src_pad_token_idx = src_vocab['<pad>']
        tgt_pad_token_idx = tgt_vocab['<pad>']
        self.source_embedding = nn.Embedding(len(src_vocab), embed_size, padding_idx=src_pad_token_idx)
        
        with torch.no_grad():
            if pretrained_source is not None:
                self.source_embedding.weight.data = pretrained_source
                self.source_embedding.weight.requires_grad = False

        self.target_embedding = self.source_embedding
        self.hidden_size = hidden_size

        # default values
        self.encoder = Encoder(
            embed_size=embed_size,
            hidden_size=hidden_size,
            source_embeddings=self.source_embedding,
        )
        self.decoder = Decoder(
            embed_size=embed_size,
            hidden_size=hidden_size,
            target_embedding=self.target_embedding,
            device=self.device,
        )


    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:
        """ Take a mini-batch of source and target sentences, compute the log-likelihood of
        target sentences under the language models learned by the SRL system.

        :param source: list of source sentence tokens
        :type source: List[List[str]]
        :param target: list of target sentence tokens, wrapped by `<s>` and `</s>`
        :type target: List[List[str]]
        :returns scores: a variable/tensor of shape (b, ) representing the
                                    log-likelihood of generating the gold-standard target sentence for
                                    each example in the input batch. Here b = batch size.
        :rtype: torch.Tensor
        """
        # Compute sentence lengths
        source_lengths = [len(s) for s in source]

        # Convert list of lists into tensors
        source_padded = self.src_vocab.to_input_tensor(source, device=self.device)   
        target_padded = self.tgt_vocab.to_input_tensor(target, device=self.device)  
        
        ###     Run the network forward:
        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`
        ###     2. Apply the decoder to compute combined-output by calling `self.decode()`
        ###     3. Compute log probability distribution over the target vocabulary using the
        ###        combined_outputs returned by the `self.decode()` function.

        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)
        combined_outputs = self.decode(enc_hiddens, dec_init_state, target_padded)
        P = F.log_softmax(self.decoder.target_vocab_projection(combined_outputs), dim=-1)

        # Zero out, probabilities for which we have nothing in the target text
        target_masks = (target_padded != self.tgt_vocab['<pad>']).float()
        
        # Compute log probability of generating true target words
        target_gold_words_log_prob = torch.gather(P, index=target_padded[:,1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[:,1:]
        scores = target_gold_words_log_prob.sum(dim=1)
        return scores


    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """ Apply the encoder to source sentences to obtain encoder hidden states.
            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.

        :param source_padded: Tensor of padded source sentences with shape (src_len, b), where
            b = batch_size, src_len = maximum source sentence length. Note that these have
            already been sorted in order of longest to shortest sentence.
        :type source_padded: torch.Tensor
        :param source_lengths: List of actual lengths for each of the source sentences in the batch
        :type source_lengths: List[int]
        :returns: Tuple of two items. The first is Tensor of hidden units with shape (b, src_len, h*2),
            where b = batch size, src_len = maximum source sentence length, h = hidden size. The second is
            Tuple of tensors representing the decoder's initial hidden state and cell.
        :rtype: Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
        """
        return self.encoder(source_padded, source_lengths)


    def decode(self, enc_hiddens: torch.Tensor, dec_init_state: Tuple[torch.Tensor, torch.Tensor], 
               target_padded: torch.Tensor) -> torch.Tensor:
        """Compute combined output vectors for a batch.

        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where
                                     b = batch size, src_len = maximum source sentence length, h = hidden size.
        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder
        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where
                                       tgt_len = maximum target sentence length, b = batch size. 

        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where
                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size
        """
        return self.decoder(enc_hiddens, dec_init_state, target_padded)


    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:
        """ Given a single source sentence, perform beam search, yielding translations in the target language.
        @param src_sent (List[str]): a single source sentence (words)
        @param beam_size (int): beam size
        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN
        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:
                value: List[str]: the decoded target sentence, represented as a list of words
                score: float: the log-likelihood of the target sentence
        """
        src_sents_var = self.src_vocab.to_input_tensor([src_sent], self.device)

        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])
        src_encodings_att_linear = self.decoder.att_projection(src_encodings)

        h_tm1 = dec_init_vec
        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device) 

        eos_id = self.tgt_vocab['</s>']

        hypotheses = [['<s>']]
        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)
        completed_hypotheses = []

        t = 0
        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:
            t += 1
            hyp_num = len(hypotheses)

            exp_src_encodings = src_encodings.expand(hyp_num,
                                                     src_encodings.size(1),
                                                     src_encodings.size(2)).to(self.device)

            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,
                                                                           src_encodings_att_linear.size(1),
                                                                           src_encodings_att_linear.size(2)).to(self.device)

            y_tm1 = torch.tensor([self.tgt_vocab[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)
            y_t_embed = self.target_embedding(y_tm1)

            x = torch.cat([y_t_embed, att_tm1], dim=-1)
          
            h_t, att_t = self.decoder.step(x, h_tm1,
                                exp_src_encodings, exp_src_encodings_att_linear)
            
            h_t, c_t = h_t

            # log probabilities over target words
            log_p_t = F.log_softmax(self.decoder.target_vocab_projection(att_t), dim=-1)

            live_hyp_num = beam_size - len(completed_hypotheses)
            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)
            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)

            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(self.tgt_vocab), rounding_mode='floor') 
            hyp_word_ids = top_cand_hyp_pos % len(self.tgt_vocab)

            new_hypotheses = []
            live_hyp_ids = []
            new_hyp_scores = []

            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):
                prev_hyp_id = prev_hyp_id.item()
                hyp_word_id = hyp_word_id.item()
                cand_new_hyp_score = cand_new_hyp_score.item()

                hyp_word = self.tgt_vocab.id2word[hyp_word_id]
                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]
                if hyp_word == '</s>':
                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],
                                                           score=cand_new_hyp_score))
                else:
                    new_hypotheses.append(new_hyp_sent)
                    live_hyp_ids.append(prev_hyp_id)
                    new_hyp_scores.append(cand_new_hyp_score)

            if len(completed_hypotheses) == beam_size:
                break

            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)
            h_tm1 = h_t[live_hyp_ids], c_t[live_hyp_ids]
            att_tm1 = att_t[live_hyp_ids]

            hypotheses = new_hypotheses
            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)

        if len(completed_hypotheses) == 0:
            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],
                                                   score=hyp_scores[0].item()))

        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)

        return completed_hypotheses

    @staticmethod
    def load(model_path: str):
        """ Load the model from a file.
        @param model_path (str): path to model
        """
        params = torch.load(model_path, map_location=lambda storage, loc: storage)
        args = params['args']
        model = SRL(
            src_vocab = params['vocab']['source'],
            tgt_vocab = params['vocab']['target'],
            # device = params['device'],
            **args
        )
        model.load_state_dict(params['state_dict'])
        return model

    def save(self, path: str):
        """ Save the odel to a file.
        @param path (str): path to the model
        """
        print('save model parameters to [%s]' % path, file=sys.stderr)

        params = {
            'args': dict(embed_size=self.embed_size, hidden_size=self.hidden_size),
            'vocab': dict(source=self.src_vocab, target=self.tgt_vocab),
            'state_dict': self.state_dict(),
            'device': self.device
        }
        torch.save(params, path)

def batch_iter(data, batch_size, shuffle=False):
    """ Yield batches of source and target sentences reverse sorted by length (largest to smallest).
    :param data: list of tuples containing source and target sentence. ie.
        (list of (src_sent, tgt_sent))
    :type data: List[Tuple[List[str], List[str]]]
    :param batch_size: batch size
    :type batch_size: int
    :param shuffle: whether to randomly shuffle the dataset
    :type shuffle: boolean
    """
    batch_num = math.ceil(len(data) / batch_size)
    index_array = list(range(len(data)))

    if shuffle:
        np.random.shuffle(index_array)

    for i in range(batch_num):
        indices = index_array[i * batch_size: (i + 1) * batch_size]
        examples = [data[idx] for idx in indices]

        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)
        src_sents = [e[0] for e in examples]
        tgt_sents = [e[1] for e in examples]

        print(f'batch size is {batch_size}')
        print(len(src_sents))

        yield src_sents, tgt_sents

def evaluate_ppl(model, val_data, batch_size=32):
    """ Evaluate perplexity on dev sentences
    :param model: SRL Model
    :type model: SRL
    :param dev_data: list of tuples containing source and target sentence.
        i.e. (list of (src_sent, tgt_sent))
    :param val_data: List[Tuple[List[str], List[str]]]
    :param batch_size: size of batches to extract
    :type batch_size: int
    :returns ppl: perplixty on val sentences
    """
    was_training = model.training
    model.eval()

    cum_loss = 0.
    cum_tgt_words = 0.

    # no_grad() signals backend to throw away all gradients
    with torch.no_grad():
        for src_sents, tgt_sents in batch_iter(val_data, batch_size):
            loss = -model(src_sents, tgt_sents).sum()

            cum_loss += loss.item()
            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`
            cum_tgt_words += tgt_word_num_to_predict

        ppl = np.exp(cum_loss / cum_tgt_words)

    if was_training:
        model.train()

    return ppl

def train_and_evaluate(model, train_data, val_data, optimizer, train_batch_size=32, clip_grad=2, log_every = 100, 
                       valid_niter = 500, model_save_path="srl.ckpt", num_epoch=6):
    num_trail = 0
    cum_examples = report_examples = epoch = valid_num = 0
    hist_valid_scores = []
    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0

    print('begin Maximum Likelihood training')
    train_time = begin_time = time.time()

    val_data_tgt = [tgt for _, tgt in val_data]

    for epoch in tqdm(range(num_epoch)):
        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):
            train_iter += 1
            
            optimizer.zero_grad()
            
            batch_size = len(src_sents)
            
            example_losses = -model(src_sents, tgt_sents)
            batch_loss = example_losses.sum()
            loss = batch_loss / batch_size
            loss.backward()
            
            # clip gradient
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
            
            optimizer.step()
            
            batch_losses_val = batch_loss.item()
            report_loss += batch_losses_val
            cum_loss += batch_losses_val
            
            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`
            report_tgt_words += tgt_words_num_to_predict
            cum_tgt_words += tgt_words_num_to_predict
            report_examples += batch_size
            cum_examples += batch_size

            if train_iter % log_every == 0:
                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \
                        'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,
                                                                                            report_loss / report_examples,
                                                                                            math.exp(report_loss / report_tgt_words),
                                                                                            cum_examples,
                                                                                            report_tgt_words / (time.time() - train_time),
                                                                                            time.time() - begin_time))
                train_time = time.time()
                report_loss = report_tgt_words = report_examples = 0.

                

            # perform validation
            if train_iter % valid_niter == 0:
                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,
                                                                                            cum_loss / cum_examples,
                                                                                            np.exp(cum_loss / cum_tgt_words),
                                                                                            cum_examples))
                
                cum_loss = cum_examples = cum_tgt_words = 0.
                valid_num += 1

                print('begin validation ...', file=sys.stderr)

                # compute dev. ppl 
                dev_ppl = evaluate_ppl(model, val_data, batch_size=128)   # dev batch size can be a bit larger
                valid_metric = -dev_ppl


                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)

                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)
                hist_valid_scores.append(valid_metric)

                if is_better:
                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)
                    model.save(model_save_path)

                    # also save the optimizers' state
                    torch.save(optimizer.state_dict(), model_save_path + '.optim')

"""Run the following cells to train and save your model:"""

embed_size = 300
hidden_size = 512

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
np.random.seed(1234)
torch.manual_seed(1234)

epochs = 8
train_batch_size = 128
clip_grad = 2
log_every = 100
valid_niter = 500
model_save_path="srl.ckpt"

model = SRL(
    embed_size,
    hidden_size,
    src_vocab,
    tgt_vocab,
    device=device,
    pretrained_source=src_embeddings
)

model.to(device)
model.train()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Define each of the variables then you can run this command!
train_and_evaluate(
    model,
    train_data,
    val_data,
    optimizer,
    train_batch_size,
    clip_grad,
    log_every,
    valid_niter,
    model_save_path,
    epochs
)

"""## 3.3 Get Entity level F1 score on the validation set"""

model = SRL.load(f"./drive/MyDrive/CS4740_HW3/srl.ckpt")
print(next(model.parameters()).device)  # check if model is on the intented device

def convert_output(biolst,input, predict_output,tag):
  word_to_idx = {word:idx for idx, word  in enumerate(input)}
  all_idx = []
  for w in predict_output:
    if w in word_to_idx:
      all_idx.append(word_to_idx[w])
  all_idx.sort()
  prev = None
  for idx in all_idx:
    if prev and prev == idx-1:
      biolst[idx] = "I-"+tag
    else:
      biolst[idx] = "B-"+tag
    prev = idx
  return biolst

#use beam search to generate output
val_data_src = [src for src, _ in val_data]
val_pred = []

t = len(val_data_src)//5
for i in range(t):
  input = val_data_src[i*5][2:-1]
  BIOlst_output = ['O'] * len(input)
  for j in range(5):
    pos = i*5+j
    s = val_data_src[pos]
    tag = s[-1]
    result = model.beam_search(
                  s,
                  beam_size=16,
                  max_decoding_time_step=len(s)
              )
    pred = result[0].value
    input = s[2:-1]
    BIOlst_output = convert_output(BIOlst_output, input, pred,tag)
  val_pred.append(BIOlst_output)

val_pred_2 = []
for i in val_pred:
  val_pred_2.extend(i)
y_pred_model2_dict = format_output_labels(val_pred_2, val_idx)
y_true_model2_dict = format_output_labels(val_true, val_idx)
print(str(mean_f1(y_pred_model2_dict, y_true_model2_dict)))

"""<a name="l5"></a>
## 3.4 Submission to Kaggle
"""

#output result from test set and submit to Kaggle
test_src_corpus = generate_source_corpus(test['text'], test['verb_index'])
test_pred = []
test_data_src = test_src_corpus

t = len(test_data_src)//5
for i in range(t):
  input = test_data_src[i*5][2:-1]
  BIOlst_output = ['O'] * len(input)
  for j in range(5):
    pos = i*5+j
    s = test_data_src[pos]
    tag = s[-1]
    result = model.beam_search(
                  s,
                  beam_size=16,
                  max_decoding_time_step=len(s)
              )
    pred = result[0].value
    input = s[2:-1]
    BIOlst_output = convert_output(BIOlst_output, input, pred,tag)
  test_pred.append(BIOlst_output)


flattened_encoder_predictions = [j for i in test_pred for j in i]
flattened_encoder_index = [j for i in test['words_indices'] for j in i]
create_submission("./drive/MyDrive/CS4740_HW3/encoder_decoder_predictions_11_12.csv", flattened_encoder_predictions, flattened_encoder_index)

"""## 3.5 Questions

<a name="Q3.1"></a>
### **Q3.1**

What are the limitations of converting semantic role labeling task to question&answer(model 2) using encoder-decoder model?

#### **A3.1**
... add your answers here

<a name="part4"></a>
# **Part 4: Analysis**

## Part 4.1: Model Comparison

Compare two models above either using quantitative or qualitative analysis.

The descriptive analysis can take one of two forms:

1. _Nuanced quantitative analysis_ \
If you choose this option, you will need to further break down the quantitative statistics you reported initially. We provide some initial strategies to prime you for what you should think about in doing this. One possible starting point is to consider: if model $X$ achieves greater accuracy than model $Y$, to what extent is $X$ getting everything correct that $Y$ gets correct? For example, what's model's performance on each semantic role types?

2. _Nuanced qualitative analysis_ \
If you choose this option, you will need to select individual examples and try to explain or reason about why one model may be getting them right whereas the other isnâ€™t. Are there any examples that both models get right or wrong and, if so, can you hypothesize a reason why this occurs?


**NOTE:** The report should be written keeping both of the models in mind, discussing both of their performances, as well as doing the nuanced analysis with both of the models.

## **A4.1**

... add your answers here

# **Baselines**

On Kaggle, we will provide two baselines for you to evaluate your models agaist: **`LSTM TA Baseline`** of 0.25074 and **`Encoder-Decoder TA Baseline`** of 0.21261. You may use them to internally check your models.

---
<a name="part5"></a>
## **Competition Score**
[[^^^]](#outline) 


Include your team's **best score** (for a valid LSTM implementation and a valid Encoder-Decoder implementation) and the **name under which that best score was submitted** from Kaggle. See CMS for full instructions.

#### **A:**

... add your answers here
"""

